import unittest
from unittest.mock import patch, MagicMock
from src.main.python.com.hsbc.cdms.dataSource.impl.Db2DataSource import Db2DataSource
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

class TestDb2DataSource(unittest.TestCase):

    def setUp(self):
        # 初始化 SparkSession
        self.spark = SparkSession.builder \
            .appName("Db2DataSourceTest") \
            .getOrCreate()

        # 创建 Db2DataSource 实例
        self.db2_data_source = Db2DataSource()

        # 配置示例
        self.config = {
            "dbConfig": {
                "class": "Db2DataSourceConnection",
                "url": "jdbc:db2://example.db2:50000/SAMPLE",
                "user": "user",
                "password": "encrypted_password"
            },
            "query": "SELECT * FROM SAMPLE_TABLE"
        }

        # 日志记录器模拟
        self.logger = MagicMock()

        # 结果模式示例
        self.schema = StructType([
            StructField("column1", StringType(), True),
            StructField("column2", StringType(), True)
        ])

    def tearDown(self):
        self.spark.stop()

    @patch('src.main.python.com.hsbc.cdms.dataSource.impl.Db2DataSourceConnection')
    def test_read_data_success(self, mock_db2_data_source_connection):
        # 模拟连接属性和 JDBC URL
        mock_db2_data_source_connection.return_value.jdbc_connection.return_value = (
            {"user": "user", "password": "password"}, "jdbc:db2://example.db2:50000/SAMPLE"
        )

        # 模拟查询结果
        expected_data = [
            ("value1", "value2"),
            ("value3", "value4")
        ]
        expected_df = self.spark.createDataFrame(expected_data, self.schema)

        # 使用 patch 模拟 DataFrameReader 的 jdbc 方法
        with patch('pyspark.sql.DataFrameReader.jdbc', return_value=expected_df) as mock_jdbc:
            # 调用 read_data 方法
            result_df = self.db2_data_source.read_data(
                self.logger, self.spark, self.config, "SAMPLE_TABLE", self.schema
            )

            # 验证结果
            self.assertEqual(result_df.collect(), expected_df.collect())

            # 验证日志记录
            self.logger.info.assert_called_with("Starting data extraction from DB2...")
            self.logger.info.assert_called_with(f"Executing query: {self.config['query']}")

    @patch('src.main.python.com.hsbc.cdms.dataSource.impl.Db2DataSourceConnection')
    @patch('src.main.python.com.hsbc.cdms.util.PasswordEncryptAndDecrypt.password_decrypt')
    def test_read_data_exception(self, mock_decrypt, mock_db2_data_source_connection):
        # 模拟连接属性和 JDBC URL
        mock_db2_data_source_connection.return_value.jdbc_connection.return_value = (
            {"user": "user", "password": "encrypted_password"}, "jdbc:db2://example.db2:50000/SAMPLE"
        )

        # 模拟 password_decrypt 返回非法字节，触发 UnicodeDecodeError
        mock_decrypt.return_value = b"\xa5"  # 模拟 ASCII 解码失败

        # 调用 read_data 方法
        with self.assertRaises(UnicodeDecodeError):
            self.db2_data_source.read_data(
                self.logger, self.spark, self.config, "SAMPLE_TABLE", self.schema
            )

        # 验证日志记录
        self.logger.error.assert_called_with("Error details: 'ascii' codec can't decode byte 0xa5 in position 0: ordinal not in range(128)")

if __name__ == '__main__':
    unittest.main()
